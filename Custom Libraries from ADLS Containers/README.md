# Custom Libraries in Databricks (DBR 15.x and Above)

This example demonstrates the **prescribed, supported approach** to loading custom Python libraries in Databricks starting with **Databricks Runtime (DBR) 15.x** and beyond.

---

## ğŸš€ Core Intent

Starting with DBR 15.x, Databricks **no longer allows**:
- `%pip install` at the cluster level  
- `dbutils.library.install()` or `installPyPI()`  
- Direct `conda install` or `pip install` on clusters

To ensure consistency, reproducibility, and stability,  
the supported methods are now:

âœ… **Notebook-scoped `%pip install`** â†’ affects only the notebook session  
âœ… **Cluster or workspace libraries** â†’ attach prebuilt `.whl` or `.egg` files via UI or cluster config  
âœ… **Init scripts (advanced)** â†’ for automated, team-wide setups

---

## ğŸ“¦ What This Example Covers

This example shows how to:
1. Build a **custom Python library** (`mylib`) as a proper, versioned package  
2. Upload the `.whl` file to Databricks (via DBFS or ADLS)  
3. Install the library **notebook-scoped** using `%pip install`  
4. Import and use it inside notebooks **without relying on deprecated methods**

---
### ğŸ“” Notebooks in This Folder

This folder includes Databricks notebooks that:
- Walk through the setup steps interactively  
- Show how to configure Spark for ADLS access  
- Demonstrate uploading `.py` files, mounting ADLS paths, and reading from them  
- Provide end-to-end flows for using the packaged library inside notebooks

âœ… **Notebooks included:**
### ğŸ“” Notebooks in This Folder

This folder includes Databricks notebooks that:
- Walk through the setup steps interactively  
- Show how to configure Spark for ADLS access  
- Demonstrate uploading `.py` files, mounting ADLS paths, and reading from them  
- Provide end-to-end flows for using the packaged library inside notebooks

âœ… **Notebooks included:**
- `00_Setup and Access-ADLS-Container from the Notebook.ipynb` â†’ Create Databricks secret, set up ADLS access, and verify connectivity  
- `01_ADLS_Library_Setup.ipynb` â†’ Prepare the custom Python library and upload it to ADLS  
- `02_ADLS_Library_Usage.ipynb` â†’ Use the uploaded library inside Databricks notebooks


âœ… These notebooks act as **step-by-step guides** and are the primary reference for running the examples in Databricks.
---

## ğŸ“ Repository Structure

```text
mylib_package/
â”œâ”€â”€ mylib/
â”‚   â”œâ”€â”€ __init__.py      # Makes the package importable
â”‚   â””â”€â”€ core.py          # Contains example functions (greet, add, multiply)
â”œâ”€â”€ setup.py             # Packaging config for setuptools
â””â”€â”€ README.md            # This documentation
```

---

## ğŸ’¡ Why This Matters

âœ… Aligns with Databricksâ€™ modern library management best practices  
âœ… Avoids manual file drops or `sys.path` hacks  
âœ… Enables reproducible, versioned deployments  
âœ… Prepares teams for cluster-wide installation via workspace libraries or automation

---

## ğŸ“š Usage Workflow (Summary)

1ï¸âƒ£ On your local machine:
```bash
pip install --upgrade setuptools wheel
python setup.py bdist_wheel
```

2ï¸âƒ£ Upload the generated `.whl` (in `dist/`) to:
- **DBFS** (e.g., `dbfs:/tmp/mylib-0.1-py3-none-any.whl`)  
- **OR ADLS container**

3ï¸âƒ£ In the Databricks notebook:
```python
%pip install /dbfs/tmp/mylib-0.1-py3-none-any.whl
```

4ï¸âƒ£ Restart the kernel:
```python
%restart_python
```

5ï¸âƒ£ Import and use:
```python
import mylib
print(mylib.greet("Anand"))
```

---

## âš™ï¸ Notes for Cluster-Wide Installation

If you want the library available across all notebooks and jobs on a cluster:
- Go to **Workspace â†’ Libraries â†’ Install New â†’ Upload**
- Upload the `.whl` file
- Attach it to the desired cluster

---

## ğŸ”— References

- [Databricks: Managing Libraries](https://learn.microsoft.com/en-us/azure/databricks/libraries/object-storage-libraries)
- [Databricks Runtime 15.x Release Notes](https://docs.databricks.com/release-notes/runtime/15.x.html)
