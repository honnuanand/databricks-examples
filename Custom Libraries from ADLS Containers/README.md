# Custom Libraries in Databricks (DBR 15.x and Above)

This example demonstrates the **prescribed, supported approach** to loading custom Python libraries in Databricks starting with **Databricks Runtime (DBR) 15.x** and beyond.

---

## ğŸš€ Core Intent

Starting with DBR 15.x, Databricks **no longer allows**:
- `%pip install` at the cluster level  
- `dbutils.library.install()` or `installPyPI()`  
- Direct `conda install` or `pip install` on clusters

To ensure consistency, reproducibility, and stability,  
the supported methods are now:

âœ… **Notebook-scoped `%pip install`** â†’ affects only the notebook session  
âœ… **Cluster or workspace libraries** â†’ attach prebuilt `.whl` or `.egg` files via UI or cluster config  
âœ… **Init scripts (advanced)** â†’ for automated, team-wide setups

> âœ… Instead, Databricks **prescribes two supported methods** for loading custom libraries:
> 1. **Workspace Files**
> 2. **External Storage (e.g., ADLS) with `%pip install` from `.whl` files**


| **Feature / Aspect**            | **Workspace Files**                                 | **ADLS (Azure Data Lake Storage)**                                                |
|-------------------------------|-----------------------------------------------------|-----------------------------------------------------------------------------------|
| ğŸ› ï¸ Setup Complexity            | âœ… Simple â€” upload via UI or CLI                    | âš ï¸ Moderate â€” requires secret scope, no mounts in Azure Gov                       |
| ğŸ” Security & Access Control    | âœ… Workspace ACLs                                   | âœ… Better â€” uses Azure IAM + secret scopes                                        |
| ğŸ“¦ Library Packaging            | âœ… Use raw `.py` files or folders                   | âœ… Supports `.whl`, `.egg` for structured, versioned deployment                   |
| ğŸ”„ Version Control              | âœ… Git-backed Repos possible                        | âŒ Manual â€” but supports CI/CD pipelines                                          |
| â™»ï¸ Reusability Across Notebooks | âœ… Yes, within workspace                            | âœ… Yes â€” portable across clusters/workspaces                                      |
| ğŸš€ Deployment Speed             | âœ… Fast for dev/test                                | âš ï¸ Copy required (no mounts in Azure Gov)                                        |
| ğŸ’¾ Persistence                  | âœ… Tied to workspace                                | âœ… Long-term, survives workspace/cluster changes                                  |
| ğŸ§ª Best for                     | Dev/test, quick iterations                         | âœ… Production, secure CI/CD, Gov cloud compliant                                  |
| âš ï¸ Limitations                 | âŒ Workspace bloat, no clean versioning             | âŒ Needs file copy (no mount), more setup overhead                                |
| ğŸ”Œ External Access / Automation | âœ… Via UI, API, or Git integration                  | âœ… Excellent for automated externalized pipelines                                 |
| ğŸ§  DBR Compatibility (15.x+)    | âœ… Fully supported                                  | âœ… Fully supported â€” use copy approach (mount not supported in Gov)              |


| **Use Case**                             | **Preferred Method**                          |
|------------------------------------------|-----------------------------------------------|
| Dev/test (any cloud)                     | âœ… Workspace Files                             |
| Production (Azure Gov or Commercial)     | âœ… ADLS with secret-based access (no mount)    |
| Secure deployment pipelines              | âœ… ADLS + `.whl` packaging                     |
| Quick script reuse within a notebook     | âœ… Workspace Files                             |
---
## Option 1 - **Workspace Files**
ğŸ“¦ Upload your .py or .whl files to:
	â€¢	Workspace Files (/Workspace/Users/<your_email>/lib_folder/)
	â€¢	These files are version-controlled, accessible via notebooks, and secure
	â€¢	Supports both .py and .whl modules
```Python
import sys
sys.path.append("/Workspace/Users/your.name@databricks.com/lib_folder")
import mylib
mylib.greet("Anand")
```
---
## Option 2 - **External Storage (e.g., ADLS) with `%pip install` from `.whl` files**
---
## ğŸ“¦ What This Example Covers

This example shows how to:
1. Build a **custom Python library** (`mylib`) as a proper, versioned package  
2. Upload the `.whl` file to Databricks (via DBFS or ADLS)  
3. Install the library **notebook-scoped** using `%pip install`  
4. Import and use it inside notebooks **without relying on deprecated methods**

---

âœ… **Notebooks included:**
### ğŸ“” Notebooks in This Folder

This folder includes Databricks notebooks that:
- Walk through the setup steps interactively  
- Show how to configure Spark for ADLS access  
- Demonstrate uploading `.py` files, mounting ADLS paths, and reading from them  
- Provide end-to-end flows for using the packaged library inside notebooks

âœ… **Notebooks included:**
- `00_Setup and Access-ADLS-Container from the Notebook.ipynb` â†’ Create Databricks secret, set up ADLS access, and verify connectivity  
- `01_ADLS_Library_Setup.ipynb` â†’ Prepare the custom Python library and upload it to ADLS  
- `02_ADLS_Library_Usage.ipynb` â†’ Use the uploaded library inside Databricks notebooks


âœ… These notebooks act as **step-by-step guides** and are the primary reference for running the examples in Databricks.
---

## ğŸ“ Repository Structure

```text
mylib_package/
â”œâ”€â”€ mylib/
â”‚   â”œâ”€â”€ __init__.py      # Makes the package importable
â”‚   â””â”€â”€ core.py          # Contains example functions (greet, add, multiply)
â”œâ”€â”€ setup.py             # Packaging config for setuptools
â””â”€â”€ README.md            # This documentation
```

---

## ğŸ’¡ Why This Matters

âœ… Aligns with Databricksâ€™ modern library management best practices  
âœ… Avoids manual file drops or `sys.path` hacks  
âœ… Enables reproducible, versioned deployments  
âœ… Prepares teams for cluster-wide installation via workspace libraries or automation

---

## ğŸ“š Usage Workflow (Summary)

1ï¸âƒ£ On your local machine:
```bash
pip install --upgrade setuptools wheel
python setup.py bdist_wheel
```

2ï¸âƒ£ Upload the generated `.whl` (in `dist/`) to:
- **DBFS** (e.g., `dbfs:/tmp/mylib-0.1-py3-none-any.whl`)  
- **OR ADLS container**

3ï¸âƒ£ In the Databricks notebook:
```python
%pip install /dbfs/tmp/mylib-0.1-py3-none-any.whl
```

4ï¸âƒ£ Restart the kernel:
```python
%restart_python
```

5ï¸âƒ£ Import and use:
```python
import mylib
print(mylib.greet("Anand"))
```

---

## âš™ï¸ Notes for Cluster-Wide Installation

If you want the library available across all notebooks and jobs on a cluster:
- Go to **Workspace â†’ Libraries â†’ Install New â†’ Upload**
- Upload the `.whl` file
- Attach it to the desired cluster

---

## ğŸ”— References

- [Databricks: Managing Libraries](https://learn.microsoft.com/en-us/azure/databricks/libraries/object-storage-libraries)
- [Databricks Runtime 15.x Release Notes](https://docs.databricks.com/release-notes/runtime/15.x.html)
