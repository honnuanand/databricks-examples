{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e78776d5-597d-4e7d-951c-cf017dcd8024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d95a39b-d86c-4472-aa1e-b67562cabd1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Architecture Context: Databricks + ADLS Gen2 (Azure Gov) Integration\n",
    "\n",
    "This notebook is part of a workflow to demonstrate:\n",
    "\n",
    "‚úÖ How Databricks integrates with Azure Data Lake Storage (ADLS) Gen2 on Azure Government cloud  \n",
    "‚úÖ Secure authentication using storage account keys (or preferably, Azure Key Vault)  \n",
    "‚úÖ How Spark reads and lists data directly from ADLS Gen2 using the `abfss://` protocol and the `.dfs.core.usgovcloudapi.net` endpoint\n",
    "\n",
    "### Key Points:\n",
    "- **ADLS Gen2** provides scalable, hierarchical storage for big data workloads.\n",
    "- **Azure Government Cloud** uses dedicated `.usgovcloudapi.net` DNS domains.\n",
    "- **Databricks Secrets** provide a secure way to manage and access sensitive credentials.\n",
    "- **Spark on Databricks** can natively connect to ADLS via the Hadoop Azure connector, using access keys, OAuth, or service principals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8cdc2f6-66bd-455e-90cf-14b4967eb10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Accessing ADLS Gen2 in Azure Government with Hardcoded Storage Account Key\n",
    "\n",
    "This notebook section demonstrates how to:\n",
    "\n",
    "‚úÖ Set up Spark configuration to access an Azure Data Lake Storage (ADLS) Gen2 container  \n",
    "‚úÖ Use the **storage account access key** directly (hardcoded)  \n",
    "‚úÖ List the contents of a specific folder (`test/`) inside the ADLS container  \n",
    "‚úÖ Verify available secret scopes using `dbutils.secrets.listScopes()`\n",
    "\n",
    "‚ö† **Warning:**  \n",
    "For production use, avoid hardcoding sensitive keys directly in notebooks.  \n",
    "Instead, store them securely in Databricks Secrets or Azure Key Vault.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91af861a-3913-4701-8ef0-7597d235c678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Name: Bridging the Digital Divide.pdf, Size: 61282, Path: abfss://araolibraryloadtest@araostorage.dfs.core.usgovcloudapi.net/test/Bridging the Digital Divide.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SecretScope(name='test-scope')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace these with your actual details\n",
    "storage_account_name = \"araostorage\"\n",
    "container_name = \"araolibraryloadtest\"\n",
    "storage_account_key = \"YOUR_ACCESS_KEY\"  # ‚ö† Be careful ‚Äî plain text!\n",
    "\n",
    "# Configure Spark to use the access key (Azure Gov cloud domain)\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.usgovcloudapi.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "# Define the ABFSS path\n",
    "path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.usgovcloudapi.net/test\"\n",
    "\n",
    "# Test listing the folder contents\n",
    "try:\n",
    "    files = dbutils.fs.ls(path)\n",
    "    for file in files:\n",
    "        print(f\"‚úÖ Name: {file.name}, Size: {file.size}, Path: {file.path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing path: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "136ef74b-111b-4286-afd6-1096bb26137e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Accessing ADLS Gen2 in Azure Government with Databricks Secrets\n",
    "\n",
    "This notebook section demonstrates how to:\n",
    "\n",
    "‚úÖ Securely configure Spark to access Azure Data Lake Storage (ADLS) Gen2  \n",
    "‚úÖ Retrieve the **storage account access key** from Databricks Secrets  \n",
    "‚úÖ Apply the key to Spark configs for authentication  \n",
    "‚úÖ List the contents of a specific folder (`test/`) inside the ADLS container\n",
    "\n",
    "‚ö† **Best Practice:**  \n",
    "Using Databricks Secrets ensures sensitive credentials are not exposed in plaintext within notebooks or logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec40cd2-fd5c-44d2-b582-20061b4fd7b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setting Up Databricks Secrets and Accessing ADLS Gen2 (Azure Gov)\n",
    "\n",
    "This section explains how to securely configure your Databricks environment to access an Azure Data Lake Storage (ADLS) Gen2 container **without hardcoding sensitive credentials**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1Ô∏è‚É£: Configure the Databricks CLI\n",
    "\n",
    "Before you can create or manage secrets, you must set up the Databricks CLI.\n",
    "\n",
    "Run this in your terminal:\n",
    "\n",
    "```\n",
    "databricks configure --token\n",
    "```\n",
    "\n",
    "‚úÖ You will be prompted to enter:\n",
    "- **Databricks Host** ‚Üí e.g., `https://<workspace>.azuredatabricks.net` (commercial) or `https://<workspace>.azure.us` (Azure Gov)  \n",
    "- **Personal Access Token (PAT)** ‚Üí Generate this from the Databricks UI ‚Üí User Settings ‚Üí Access Tokens\n",
    "\n",
    "This will create a `~/.databrickscfg` file to let the CLI communicate with your Databricks workspace.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2Ô∏è‚É£: Use the Script Below to Create the Secret\n",
    "\n",
    "Use this shell script to:\n",
    "‚úÖ Check if the secret scope exists (creates if missing)  \n",
    "‚úÖ Check if the secret key exists (updates or creates as needed)  \n",
    "‚úÖ Store the ADLS storage account key securely inside Databricks Secrets\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Databricks Secret Setup Script (Interactive + Param-Driven)\n",
    "# This script:\n",
    "# ‚úÖ Accepts optional CLI parameters for scope, key name, and secret value\n",
    "# ‚úÖ If parameters are missing, prompts the user interactively\n",
    "# ‚úÖ Checks if the scope exists (creates if missing)\n",
    "# ‚úÖ Checks if the key exists (updates if present, adds if missing)\n",
    "# ‚úÖ Stores or updates the secret securely\n",
    "#\n",
    "# Usage:\n",
    "# ./setup_secret.sh <scope_name> <secret_key_name> <secret_value>\n",
    "#\n",
    "# Example:\n",
    "# ./setup_secret.sh adls-secrets adls-access-key AbC1234xyz==\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Get parameters or prompt interactively\n",
    "SCOPE_NAME=$1\n",
    "SECRET_KEY_NAME=$2\n",
    "SECRET_VALUE=$3\n",
    "\n",
    "if [ -z \"$SCOPE_NAME\" ]; then\n",
    "    read -p \"Enter the secret scope name: \" SCOPE_NAME\n",
    "fi\n",
    "\n",
    "if [ -z \"$SECRET_KEY_NAME\" ]; then\n",
    "    read -p \"Enter the secret key name: \" SECRET_KEY_NAME\n",
    "fi\n",
    "\n",
    "if [ -z \"$SECRET_VALUE\" ]; then\n",
    "    read -s -p \"Enter the secret value (input hidden): \" SECRET_VALUE\n",
    "    echo\n",
    "fi\n",
    "\n",
    "# Check if scope exists\n",
    "echo \"üîç Checking if secret scope '$SCOPE_NAME' exists...\"\n",
    "if databricks secrets list-scopes | grep -q \"$SCOPE_NAME\"; then\n",
    "    echo \"‚úÖ Secret scope '$SCOPE_NAME' already exists.\"\n",
    "else\n",
    "    echo \"‚öôÔ∏è Creating secret scope '$SCOPE_NAME'...\"\n",
    "    databricks secrets create-scope $SCOPE_NAME\n",
    "    echo \"‚úÖ Secret scope '$SCOPE_NAME' created.\"\n",
    "fi\n",
    "\n",
    "# Check if secret key exists\n",
    "if databricks secrets list-secrets $SCOPE_NAME | grep -q \"$SECRET_KEY_NAME\"; then\n",
    "    echo \"‚ôªÔ∏è Secret key '$SECRET_KEY_NAME' already exists ‚Äî updating it.\"\n",
    "else\n",
    "    echo \"‚ûï Secret key '$SECRET_KEY_NAME' does not exist ‚Äî creating it.\"\n",
    "fi\n",
    "\n",
    "# Store or update the secret\n",
    "echo -n \"$SECRET_VALUE\" | databricks secrets put-secret $SCOPE_NAME $SECRET_KEY_NAME\n",
    "\n",
    "# Final list\n",
    "echo \"üìã Final list of secrets in scope '$SCOPE_NAME':\"\n",
    "databricks secrets list-secrets $SCOPE_NAME\n",
    "\n",
    "echo \"üöÄ Done! You can now access the secret in your Databricks notebooks using dbutils.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3Ô∏è‚É£: Use the Secret in Your Notebook\n",
    "\n",
    "In your notebook, use the following approach to securely access ADLS:\n",
    "\n",
    "```\n",
    "# Load the storage account key from Databricks Secrets\n",
    "storage_account = \"araostorage\"\n",
    "container_name = \"araolibraryloadtest\"\n",
    "scope = \"adls-secrets\"\n",
    "key_name = \"adls-access-key\"\n",
    "\n",
    "# Fetch and clean the key\n",
    "storage_account_key = dbutils.secrets.get(scope=scope, key=key_name).strip()\n",
    "\n",
    "# Apply Spark config for Azure Gov\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.usgovcloudapi.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "# List contents inside the container or specific folder\n",
    "try:\n",
    "    files = dbutils.fs.ls(f\"abfss://{container_name}@{storage_account}.dfs.core.usgovcloudapi.net/\")\n",
    "    for file in files:\n",
    "        print(f\"‚úÖ Name: {file.name}, Size: {file.size} bytes, Path: {file.path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing ADLS: {e}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö† Important Best Practices\n",
    "\n",
    "- Never print or log the full secret value directly in notebooks.\n",
    "- Always use Databricks Secrets or Key Vault integration in production.\n",
    "- For Azure Government, ensure you are using the `.dfs.core.usgovcloudapi.net` endpoint.\n",
    "\n",
    "Once the above steps are complete, you will have:\n",
    "‚úÖ A secure, reusable secret  \n",
    "‚úÖ Spark configs wired to access ADLS Gen2  \n",
    "‚úÖ A tested path to list and process files in your Azure Gov environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1a25ff-f7cd-4205-aba9-7f2a7dffc501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Retrieved secret value: [REDACTED]\n"
     ]
    }
   ],
   "source": [
    "# Define the scope and key\n",
    "scope = \"adls-secrets\"\n",
    "key_name = \"adls-access-key\"\n",
    "\n",
    "# Fetch the secret value\n",
    "secret_value = dbutils.secrets.get(scope=scope, key=key_name)\n",
    "\n",
    "# ‚ö† WARNING: This will print the secret value in logs (use only for debugging)\n",
    "print(f\"üîç Retrieved secret value: {secret_value}\") #<--- will be redacted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f267664-d110-4036-8962-afdb5d874014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Accessing and Listing Files in ADLS Gen2 (Azure Gov) Using Databricks Secrets\n",
    "\n",
    "This notebook cell demonstrates how to:\n",
    "\n",
    "‚úÖ Load the ADLS storage account access key securely from a Databricks Secret Scope (`adls-secrets`)  \n",
    "‚úÖ Apply the Spark configuration to authenticate to the Azure Government ADLS endpoint (`.dfs.core.usgovcloudapi.net`)  \n",
    "‚úÖ List the files and folders at the **root level** of the specified ADLS container (`araolibraryloadtest`)  \n",
    "‚úÖ Additionally, list the contents specifically inside the `test/` folder within that container\n",
    "\n",
    "‚ö† **Important:**  \n",
    "- This approach avoids hardcoding sensitive keys directly in the notebook.  \n",
    "- Always use `.strip()` on the retrieved secret to avoid hidden whitespace issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1470eaac-d184-478e-bd7e-513340756254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ test/ (0 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Load ADLS storage account key from Databricks Secrets\n",
    "storage_account = \"araostorage\"\n",
    "container_name = \"araolibraryloadtest\"\n",
    "scope = \"adls-secrets\"\n",
    "key_name = \"adls-access-key\"\n",
    "\n",
    "# Fetch and clean the key\n",
    "storage_account_key = dbutils.secrets.get(scope=scope, key=key_name).strip()\n",
    "\n",
    "# Apply Spark config for Azure Gov\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.usgovcloudapi.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "# Test: list files in ADLS container\n",
    "try:\n",
    "    files = dbutils.fs.ls(f\"abfss://{container_name}@{storage_account}.dfs.core.usgovcloudapi.net/\")\n",
    "    for file in files:\n",
    "        print(f\"‚úÖ {file.name} ({file.size} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing ADLS: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "folder_path = f\"abfss://{container_name}@{storage_account}.dfs.core.usgovcloudapi.net/test\"\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(folder_path)\n",
    "    for file in files:\n",
    "        print(f\"‚úÖ Name: {file.name}, Size: {file.size} bytes, Path: {file.path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing folder '{folder_path}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc340b0-66ed-4a13-a8fc-5d4ea63d619b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Recursively Listing All Files and Folders in an ADLS Gen2 Container\n",
    "\n",
    "This notebook section demonstrates how to:\n",
    "\n",
    "‚úÖ Walk through all folders and subfolders within the ADLS container  \n",
    "‚úÖ List all files, along with their names, sizes, and paths  \n",
    "‚úÖ Handle nested directory structures efficiently\n",
    "\n",
    "‚ö† **Note:**  \n",
    "Recursive listing is useful for audits or large data sweeps but can be slow on very large directories.\n",
    "Consider filtering by file type, size, or last modified date if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d16ca60-2f5d-4390-aee7-e419b215aa02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up base path\n",
    "base_path = f\"abfss://{container_name}@{storage_account}.dfs.core.usgovcloudapi.net/\"\n",
    "\n",
    "try:\n",
    "    # List root-level folders/files\n",
    "    root_items = dbutils.fs.ls(base_path)\n",
    "    for item in root_items:\n",
    "        if item.isDir():\n",
    "            print(f\"\\nüìÅ Folder: {item.name}\")\n",
    "            folder_path = item.path\n",
    "\n",
    "            # List contents inside the folder\n",
    "            try:\n",
    "                sub_items = dbutils.fs.ls(folder_path)\n",
    "                for sub_item in sub_items:\n",
    "                    print(f\"   ‚úÖ {sub_item.name} ({sub_item.size} bytes)\")\n",
    "            except Exception as sub_e:\n",
    "                print(f\"   ‚ùå Error reading folder {folder_path}: {sub_e}\")\n",
    "        else:\n",
    "            print(f\"üìÑ File at root: {item.name} ({item.size} bytes)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing base path: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74019ab-afbf-46cf-b1b9-0b4be3103363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare hardcoded vs. secret-loaded key lengths\n",
    "hardcoded_key = \"YOUR_ACCESS_KEY\".strip()  # Replace with your actual hardcoded key\n",
    "secret_key = dbutils.secrets.get(scope=\"adls-secrets\", key=\"adls-access-key\").strip()\n",
    "\n",
    "print(f\"Hardcoded key length: {len(hardcoded_key)}\")\n",
    "print(f\"Secret key length: {len(secret_key)}\")\n",
    "\n",
    "if hardcoded_key == secret_key:\n",
    "    print(\"‚úÖ Keys match exactly\")\n",
    "else:\n",
    "    print(\"‚ùå Keys differ ‚Äî investigate secret storage\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Access-ADLS-Container from the Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
